{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":800230,"sourceType":"datasetVersion","datasetId":1305},{"sourceId":8146288,"sourceType":"datasetVersion","datasetId":4817285},{"sourceId":166320370,"sourceType":"kernelVersion"}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport plotly.express as px\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer \nnltk.download('wordnet')\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nimport matplotlib.cm as cm\nfrom matplotlib import rcParams\nfrom collections import Counter\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\nimport string\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, GlobalMaxPooling1D, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:31:09.346813Z","iopub.execute_input":"2024-04-19T21:31:09.347215Z","iopub.status.idle":"2024-04-19T21:31:27.318887Z","shell.execute_reply.started":"2024-04-19T21:31:09.347179Z","shell.execute_reply":"2024-04-19T21:31:27.318071Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"2024-04-19 21:31:16.048903: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-19 21:31:16.049060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-19 21:31:16.225723: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ncolumn_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\ndf = pd.read_csv(\"/kaggle/input/sentiment-analysis-dataset/training.1600000.processed.noemoticon.csv\",encoding=\"ISO-8859-1\", names=column_names)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:31:27.320574Z","iopub.execute_input":"2024-04-19T21:31:27.321157Z","iopub.status.idle":"2024-04-19T21:31:35.119681Z","shell.execute_reply.started":"2024-04-19T21:31:27.321131Z","shell.execute_reply":"2024-04-19T21:31:35.118764Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   target          id                          date      flag  \\\n0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n\n              user                                               text  \n0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n1    scotthamilton  is upset that he can't update his Facebook by ...  \n2         mattycus  @Kenichan I dived many times for the ball. Man...  \n3          ElleCTF    my whole body feels itchy and like its on fire   \n4           Karoli  @nationwideclass no, it's not behaving at all....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>id</th>\n      <th>date</th>\n      <th>flag</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data = df[[\"target\",\"text\"]]\ndata['target'][data['target']==4]=1\ndata['text']=data['text'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:31:35.120839Z","iopub.execute_input":"2024-04-19T21:31:35.121132Z","iopub.status.idle":"2024-04-19T21:31:35.751778Z","shell.execute_reply.started":"2024-04-19T21:31:35.121108Z","shell.execute_reply":"2024-04-19T21:31:35.750945Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"stopwords_list = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:31:35.753800Z","iopub.execute_input":"2024-04-19T21:31:35.754114Z","iopub.status.idle":"2024-04-19T21:31:35.765218Z","shell.execute_reply.started":"2024-04-19T21:31:35.754088Z","shell.execute_reply":"2024-04-19T21:31:35.764357Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"STOPWORDS = set(stopwords_list)\ndef cleaning_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndata['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:31:35.766248Z","iopub.execute_input":"2024-04-19T21:31:35.766487Z","iopub.status.idle":"2024-04-19T21:31:41.900770Z","shell.execute_reply.started":"2024-04-19T21:31:35.766465Z","shell.execute_reply":"2024-04-19T21:31:41.899985Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def cleaning_email(data):\n    return re.sub('@[^\\s]+', ' ', data)\n\ndata['text']= data['text'].apply(lambda x: cleaning_email(x))\ndata['text'].head()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:31:41.901777Z","iopub.execute_input":"2024-04-19T21:31:41.902089Z","iopub.status.idle":"2024-04-19T21:31:45.164833Z","shell.execute_reply.started":"2024-04-19T21:31:41.902058Z","shell.execute_reply":"2024-04-19T21:31:45.163903Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0      http://twitpic.com/2y1zl - awww, that's bumm...\n1    upset can't update facebook texting it... migh...\n2      dived many times ball. managed save 50% rest...\n3                     whole body feels itchy like fire\n4      no, behaving all. i'm mad. here? can't see t...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"def cleaning_URLs(data):\n    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)\ndata['text'] = data['text'].apply(lambda x: cleaning_URLs(x))","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:31:45.165985Z","iopub.execute_input":"2024-04-19T21:31:45.166291Z","iopub.status.idle":"2024-04-19T21:31:56.795809Z","shell.execute_reply.started":"2024-04-19T21:31:45.166267Z","shell.execute_reply":"2024-04-19T21:31:56.794968Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"english_punctuations = string.punctuation\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', english_punctuations)\n    return text.translate(translator)\ndata['text'] = data['text'].apply(lambda text: cleaning_punctuations(text))","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:31:56.796902Z","iopub.execute_input":"2024-04-19T21:31:56.797189Z","iopub.status.idle":"2024-04-19T21:32:03.067070Z","shell.execute_reply.started":"2024-04-19T21:31:56.797166Z","shell.execute_reply":"2024-04-19T21:32:03.066244Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def cleaning_numbers(data):\n    return re.sub('[0-9]+', '', data)\ndata['text'] = data['text'].apply(lambda x: cleaning_numbers(x))","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:32:03.068468Z","iopub.execute_input":"2024-04-19T21:32:03.068733Z","iopub.status.idle":"2024-04-19T21:32:08.321331Z","shell.execute_reply.started":"2024-04-19T21:32:03.068710Z","shell.execute_reply":"2024-04-19T21:32:08.320499Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data['text'] = data['text'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:32:08.322497Z","iopub.execute_input":"2024-04-19T21:32:08.322800Z","iopub.status.idle":"2024-04-19T21:32:08.420090Z","shell.execute_reply.started":"2024-04-19T21:32:08.322774Z","shell.execute_reply":"2024-04-19T21:32:08.418897Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'\\w+')\ndata['text'] = data['text'].apply(tokenizer.tokenize)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:32:08.423458Z","iopub.execute_input":"2024-04-19T21:32:08.423761Z","iopub.status.idle":"2024-04-19T21:32:17.501483Z","shell.execute_reply.started":"2024-04-19T21:32:08.423734Z","shell.execute_reply":"2024-04-19T21:32:17.500673Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nnltk.download('omw-1.4')\ndef lemmatizer_on_text(words):\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\ndata['text'] = data['text'].apply(lambda x: lemmatizer_on_text(x))","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:32:17.502704Z","iopub.execute_input":"2024-04-19T21:32:17.502990Z","iopub.status.idle":"2024-04-19T21:32:20.413828Z","shell.execute_reply.started":"2024-04-19T21:32:17.502965Z","shell.execute_reply":"2024-04-19T21:32:20.412375Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m----> 7\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatizer_on_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n","File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m     lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m----> 7\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mlemmatizer_on_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n","Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36mlemmatizer_on_text\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatizer_on_text\u001b[39m(words):\n\u001b[1;32m      5\u001b[0m     lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n","Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatizer_on_text\u001b[39m(words):\n\u001b[1;32m      5\u001b[0m     lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/stem/wordnet.py:40\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, pos\u001b[38;5;241m=\u001b[39mNOUN):\n\u001b[0;32m---> 40\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"],"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","output_type":"error"}]},{"cell_type":"code","source":"texts=data.text\nlabels=data.target\nmax_len = 500\ntok = Tokenizer(num_words=2000)\ntok.fit_on_texts(texts)\nsequences = tok.texts_to_sequences(texts)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:45:34.817336Z","iopub.execute_input":"2024-04-19T21:45:34.817741Z","iopub.status.idle":"2024-04-19T21:46:06.268990Z","shell.execute_reply.started":"2024-04-19T21:45:34.817706Z","shell.execute_reply":"2024-04-19T21:46:06.267953Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(tok.word_index)\nif vocab_size > 2000:\n    vocab_size = 2000\n\nsequence_length = max_len\n\nprint(\"Vocabulary Size:\", vocab_size)\nprint(\"Sequence Length:\", sequence_length)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:46:06.270805Z","iopub.execute_input":"2024-04-19T21:46:06.271126Z","iopub.status.idle":"2024-04-19T21:46:06.276472Z","shell.execute_reply.started":"2024-04-19T21:46:06.271100Z","shell.execute_reply":"2024-04-19T21:46:06.275540Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Vocabulary Size: 2000\nSequence Length: 500\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline \nnum_classes = 2 \n\n# Tokenize text\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\npadded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=sequence_length, padding='post')\n\nlabels = np.array(labels)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\nbatch_size = 64\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).shuffle(buffer_size=1024)\nval_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n\ntranslation_pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n\n# Create a function to translate text to English\ndef translate_to_english(text, src_lang):\n    translated_text = translation_pipe(text, src_lang=src_lang, tgt_lang='en')\n    return translated_text[0]['translation_text']\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:46:06.277672Z","iopub.execute_input":"2024-04-19T21:46:06.277928Z","iopub.status.idle":"2024-04-19T21:46:58.014192Z","shell.execute_reply.started":"2024-04-19T21:46:06.277906Z","shell.execute_reply":"2024-04-19T21:46:58.013360Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"953069f3b45745db8a1a767ef170c313"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/310M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f07e027f8b1e4f40b8a7a9b4d60b2169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96e73a526473438d8761b88c02db2d89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e87ccf81ce14495928dfd4f7da6c4bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8508f340ecf34243a5f9a358e139dd46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/791k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baaba35c07c54cfea87f4655f447991f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de865ffaad984993a68db4ba92f36628"}},"metadata":{}}]},{"cell_type":"code","source":"class TransformerEncoderSentiment(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, dense_dim, dropout=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dense_dim = dense_dim\n        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = tf.keras.Sequential([\n            tf.keras.layers.Dense(dense_dim, activation='relu'),\n            tf.keras.layers.Dense(embed_dim),\n        ])\n        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n        self.dropout_1 = tf.keras.layers.Dropout(dropout)\n        self.dropout_2 = tf.keras.layers.Dropout(dropout)\n    \n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=tf.int32)\n        else:\n            padding_mask = None\n        \n        attention_output = self.attention(inputs, inputs, attention_mask=padding_mask)\n        proj_input = self.layernorm_1(inputs + self.dropout_1(attention_output))\n        \n        proj_output = self.dense_proj(proj_input)\n        encoder_output = self.layernorm_2(proj_input + self.dropout_2(proj_output))\n        \n        return encoder_output\n\n# Define model architecture\nembed_dim = 128\nnum_heads = 4\ndense_dim = 256\ndropout_rate = 0.1\n\ninputs = tf.keras.layers.Input(shape=(sequence_length,), dtype=tf.int32)\nembedding_layer = tf.keras.layers.Embedding(vocab_size, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerEncoderSentiment(embed_dim, num_heads, dense_dim, dropout_rate)\nx = transformer_block(x)\nx = tf.keras.layers.GlobalMaxPooling1D()(x)\nx = tf.keras.layers.Dropout(dropout_rate)(x)\noutputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:46:58.016039Z","iopub.execute_input":"2024-04-19T21:46:58.016653Z","iopub.status.idle":"2024-04-19T21:46:58.331829Z","shell.execute_reply.started":"2024-04-19T21:46:58.016625Z","shell.execute_reply":"2024-04-19T21:46:58.330990Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m256,000\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_encoder_sentiment   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m330,240\u001b[0m │\n│ (\u001b[38;5;33mTransformerEncoderSentiment\u001b[0m)   │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ transformer_encoder_sentiment   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">330,240</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoderSentiment</span>)   │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m586,498\u001b[0m (2.24 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">586,498</span> (2.24 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m586,498\u001b[0m (2.24 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">586,498</span> (2.24 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"epochs = 5\n\n# Train the model\nmodel.fit(train_ds, epochs=epochs, validation_data=val_ds)\n\n# Evaluate the model\nloss, accuracy = model.evaluate(val_ds)\nprint(\"Validation Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:46:58.332716Z","iopub.execute_input":"2024-04-19T21:46:58.332965Z","iopub.status.idle":"2024-04-19T23:42:58.201215Z","shell.execute_reply.started":"2024-04-19T21:46:58.332943Z","shell.execute_reply":"2024-04-19T23:42:58.200303Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m    2/20000\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27:30\u001b[0m 83ms/step - accuracy: 0.5039 - loss: 1.5856  ","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1713563231.774268      85 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1713563231.791475      85 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1377s\u001b[0m 68ms/step - accuracy: 0.7412 - loss: 0.5193 - val_accuracy: 0.7603 - val_loss: 0.4887\nEpoch 2/5\n\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1367s\u001b[0m 68ms/step - accuracy: 0.7600 - loss: 0.4893 - val_accuracy: 0.7614 - val_loss: 0.4870\nEpoch 3/5\n\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1370s\u001b[0m 68ms/step - accuracy: 0.7634 - loss: 0.4843 - val_accuracy: 0.7639 - val_loss: 0.4829\nEpoch 4/5\n\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1367s\u001b[0m 68ms/step - accuracy: 0.7645 - loss: 0.4813 - val_accuracy: 0.7637 - val_loss: 0.4829\nEpoch 5/5\n\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1367s\u001b[0m 68ms/step - accuracy: 0.7664 - loss: 0.4788 - val_accuracy: 0.7640 - val_loss: 0.4825\n\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 22ms/step - accuracy: 0.7646 - loss: 0.4818\nValidation Accuracy: 0.7640030980110168\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Save the model using tf.saved_model.save()\ntf.saved_model.save(model, \"sentiment_analysis_model\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T23:46:43.310246Z","iopub.execute_input":"2024-04-19T23:46:43.311147Z","iopub.status.idle":"2024-04-19T23:46:44.412209Z","shell.execute_reply.started":"2024-04-19T23:46:43.311114Z","shell.execute_reply":"2024-04-19T23:46:44.411394Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"tokenizer_config = tokenizer.to_json()\nwith open(\"tokenizer_config.json\", \"w\") as tokenizer_file:\n    tokenizer_file.write(tokenizer_config)\n\n# Define a function to predict sentiment for text in any language\ndef predict_sentiment(text, src_lang):\n    # Translate text to English\n    english_text = translate_to_english(text, src_lang)\n    \n    # Preprocess the text\n    sequence = tokenizer.texts_to_sequences([english_text])\n    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=sequence_length, padding='post')\n    \n    # Make prediction\n    predicted_probabilities = model.predict(padded_sequence)\n    predicted_class = np.argmax(predicted_probabilities)\n    \n    # Map predicted class to sentiment label\n    sentiment_label = \"Positive\" if predicted_class == 1 else \"Negative\"\n    \n    return sentiment_label, predicted_probabilities\n\n# Define a sample text in different languages\nsample_texts = {\n    'fr': 'Je suis heureux.',\n    'es': 'Estoy feliz.',\n    'de': 'Ich bin glücklich.',\n    'it': 'Sono felice.',\n    'zh-cn': '我今天很伤心.'\n}\n\n# Predict sentiment for each sample text\nfor lang, text in sample_texts.items():\n    sentiment, probabilities = predict_sentiment(text, lang)\n    print(f\"Input ({lang}): {text}\")\n    print(f\"Predicted Sentiment: {sentiment}\")\n    print(f\"Predicted Probabilities: {probabilities}\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-04-19T23:46:46.189928Z","iopub.execute_input":"2024-04-19T23:46:46.190276Z","iopub.status.idle":"2024-04-19T23:46:50.833048Z","shell.execute_reply.started":"2024-04-19T23:46:46.190251Z","shell.execute_reply":"2024-04-19T23:46:50.832093Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851ms/step\nInput (fr): Je suis heureux.\nPredicted Sentiment: Positive\nPredicted Probabilities: [[0.11296368 0.88703626]]\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\nInput (es): Estoy feliz.\nPredicted Sentiment: Positive\nPredicted Probabilities: [[0.11296368 0.88703626]]\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\nInput (de): Ich bin glücklich.\nPredicted Sentiment: Positive\nPredicted Probabilities: [[0.11296368 0.88703626]]\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\nInput (it): Sono felice.\nPredicted Sentiment: Positive\nPredicted Probabilities: [[0.11296368 0.88703626]]\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\nInput (zh-cn): 我今天很伤心.\nPredicted Sentiment: Negative\nPredicted Probabilities: [[0.95004827 0.04995171]]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Zip the model files\n!zip -r sentiment_analysis_model.zip sentiment_analysis_model tokenizer_config.json\n\n# Move the zipped file to the output directory\nimport shutil\nshutil.move(\"sentiment_analysis_model.zip\", \"/kaggle/working/sentiment_analysis_model.zip\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:32:20.427181Z","iopub.status.idle":"2024-04-19T21:32:20.427626Z","shell.execute_reply.started":"2024-04-19T21:32:20.427397Z","shell.execute_reply":"2024-04-19T21:32:20.427416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a sample English text for prediction\nsample_text = \"I loved the movie! It was fantastic.\"\n\n# Preprocess the sample text\nsample_sequence = tokenizer.texts_to_sequences([sample_text])\nsample_padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sample_sequence, maxlen=sequence_length, padding='post')\n\n# Make the prediction\npredicted_probabilities = model.predict(sample_padded_sequence)\npredicted_class = np.argmax(predicted_probabilities)\n\n# Map the predicted class to sentiment label\nsentiment_label = \"Positive\" if predicted_class == 1 else \"Negative\"\n\nprint(\"Sample Text:\", sample_text)\nprint(\"Predicted Sentiment:\", sentiment_label)\nprint(\"Predicted Probabilities:\", predicted_probabilities)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:32:20.428764Z","iopub.status.idle":"2024-04-19T21:32:20.429211Z","shell.execute_reply.started":"2024-04-19T21:32:20.428967Z","shell.execute_reply":"2024-04-19T21:32:20.428986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tkinter as tk\nfrom tkinter import ttk\nfrom transformers import pipeline\n\n# Load translation pipeline\ntranslation_pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n\n# Define language codes and names\nlanguages = {\n    'af': 'Afrikaans',\n    'ar': 'Arabic',\n    'bg': 'Bulgarian',\n    'bn': 'Bengali',\n    'ca': 'Catalan',\n    'cs': 'Czech',\n    'cy': 'Welsh',\n    'da': 'Danish',\n    'de': 'German',\n    'el': 'Greek',\n    'en': 'English',\n    'es': 'Spanish',\n    'et': 'Estonian',\n    'fa': 'Persian',\n    'fi': 'Finnish',\n    'fr': 'French',\n    'gu': 'Gujarati',\n    'he': 'Hebrew',\n    'hi': 'Hindi',\n    'hr': 'Croatian',\n    'hu': 'Hungarian',\n    'id': 'Indonesian',\n    'it': 'Italian',\n    'ja': 'Japanese',\n    'kn': 'Kannada',\n    'ko': 'Korean',\n    'lt': 'Lithuanian',\n    'lv': 'Latvian',\n    'mk': 'Macedonian',\n    'ml': 'Malayalam',\n    'mr': 'Marathi',\n    'ne': 'Nepali',\n    'nl': 'Dutch',\n    'no': 'Norwegian',\n    'pa': 'Punjabi',\n    'pl': 'Polish',\n    'pt': 'Portuguese',\n    'ro': 'Romanian',\n    'ru': 'Russian',\n    'si': 'Sinhala',\n    'sk': 'Slovak',\n    'sl': 'Slovenian',\n    'so': 'Somali',\n    'sq': 'Albanian',\n    'sv': 'Swedish',\n    'sw': 'Swahili',\n    'ta': 'Tamil',\n    'te': 'Telugu',\n    'th': 'Thai',\n    'tl': 'Tagalog',\n    'tr': 'Turkish',\n    'uk': 'Ukrainian',\n    'ur': 'Urdu',\n    'vi': 'Vietnamese',\n    'zh-cn': 'Chinese (Simplified)',\n    'zh-tw': 'Chinese (Traditional)'\n}\n\n# Function to translate text to English\ndef translate_to_english(text, src_lang):\n    translated_text = translation_pipe(text, src_lang=src_lang, tgt_lang='en')\n    return translated_text[0]['translation_text']\n\n# Function to perform sentiment analysis\ndef predict_sentiment(text, src_lang):\n    # Translate text to English\n    english_text = translate_to_english(text, src_lang)\n    \n    # Preprocess the text\n    sequence = tokenizer.texts_to_sequences([english_text])\n    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=sequence_length, padding='post')\n    \n    # Make prediction\n    predicted_probabilities = model.predict(padded_sequence)\n    predicted_class = np.argmax(predicted_probabilities)\n    \n    # Map predicted class to sentiment label\n    sentiment_label = \"Positive\" if predicted_class == 1 else \"Negative\"\n    \n    return sentiment_label, predicted_probabilities\n\n# Function to handle button click event\ndef analyze_sentiment():\n    input_text = text_entry.get(\"1.0\", \"end-1c\")\n    src_lang = lang_combobox.get()\n    \n    sentiment, probabilities = predict_sentiment(input_text, src_lang)\n    \n    result_text.set(f\"Sentiment: {sentiment}\\nProbabilities: {probabilities}\")\n\n# Create main window\nroot = tk.Tk()\nroot.title(\"Sentiment Analysis\")\n\n# Create input label and text entry\ninput_label = ttk.Label(root, text=\"Enter text:\")\ninput_label.grid(row=0, column=0, padx=10, pady=5, sticky=\"w\")\n\ntext_entry = tk.Text(root, height=5, width=40)\ntext_entry.grid(row=0, column=1, padx=10, pady=5)\n\n# Create language selection combobox\nlang_label = ttk.Label(root, text=\"Select language:\")\nlang_label.grid(row=1, column=0, padx=10, pady=5, sticky=\"w\")\n\nlanguage_codes = list(languages.keys())\nlanguage_names = list(languages.values())\nlang_combobox = ttk.Combobox(root, values=language_names, state=\"readonly\")\nlang_combobox.current(0)  # Set default language\nlang_combobox.grid(row=1, column=1, padx=10, pady=5)\n\n# Create analyze button\nanalyze_button = ttk.Button(root, text=\"Analyze Sentiment\", command=analyze_sentiment)\nanalyze_button.grid(row=2, column=0, columnspan=2, padx=10, pady=5)\n\n# Create label for displaying result\nresult_text = tk.StringVar()\nresult_label = ttk.Label(root, textvariable=result_text)\nresult_label.grid(row=3, column=0, columnspan=2, padx=10, pady=5)\n\nroot.mainloop()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T21:32:20.432782Z","iopub.status.idle":"2024-04-19T21:32:20.433184Z","shell.execute_reply.started":"2024-04-19T21:32:20.432982Z","shell.execute_reply":"2024-04-19T21:32:20.432997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}